{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a808a0-c5c1-4886-87ef-ef7a8f20a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO BASELINE: LightGBM + SMOTE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Cargando datos preprocesados...\n",
      " Cargado desde pickle\n",
      "\n",
      "  Train: (54961, 192)\n",
      "  Test: (13741, 192)\n",
      "  Features: 192\n",
      "\n",
      "  Distribución Train:\n",
      "    Clase 0: 41,280 (75.1%)\n",
      "    Clase 1: 13,681 (24.9%)\n",
      "\n",
      "[2/5] Balanceo con SMOTE...\n",
      "\n",
      "  Antes de SMOTE:\n",
      "    Clase 0: 41,280\n",
      "    Clase 1: 13,681\n",
      "    Ratio: 1:3.02\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SMOTE' object has no attribute '_validate_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Aplicar SMOTE\u001b[39;00m\n\u001b[1;32m     75\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, k_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m X_train_smote, y_train_smote \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Después de SMOTE:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Clase 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(y_train_smote\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:106\u001b[0m, in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:161\u001b[0m, in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SMOTE' object has no attribute '_validate_data'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MODELO BASELINE: LightGBM + SMOTE\n",
    "Entrenar, evaluar y guardar modelos\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, accuracy_score)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODELO BASELINE: LightGBM + SMOTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "OUTPUT_DIR = 'output_ml'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 1: CARGAR DATOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[1/5] Cargando datos preprocesados...\")\n",
    "\n",
    "# Intentar cargar desde pickle primero\n",
    "try:\n",
    "    with open(f'{OUTPUT_DIR}/train_test_split.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "    feature_names = data['feature_names']\n",
    "    \n",
    "    print(f\" Cargado desde pickle\")\n",
    "except:\n",
    "    # Cargar desde CSV\n",
    "    print(f\"  Cargando desde CSV...\")\n",
    "    X_train = pd.read_csv(f'{OUTPUT_DIR}/X_train.csv')\n",
    "    X_test = pd.read_csv(f'{OUTPUT_DIR}/X_test.csv')\n",
    "    y_train = pd.read_csv(f'{OUTPUT_DIR}/y_train.csv').squeeze()\n",
    "    y_test = pd.read_csv(f'{OUTPUT_DIR}/y_test.csv').squeeze()\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    print(f\"  Cargado desde CSV\")\n",
    "\n",
    "print(f\"\\n  Train: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\n  Distribución Train:\")\n",
    "print(f\"    Clase 0: {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"    Clase 1: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 2: BALANCEO CON SMOTE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[2/5] Balanceo con SMOTE...\")\n",
    "\n",
    "print(f\"\\n  Antes de SMOTE:\")\n",
    "print(f\"    Clase 0: {(y_train==0).sum():,}\")\n",
    "print(f\"    Clase 1: {(y_train==1).sum():,}\")\n",
    "print(f\"    Ratio: 1:{(y_train==0).sum()/(y_train==1).sum():.2f}\")\n",
    "\n",
    "# Aplicar SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\n  Después de SMOTE:\")\n",
    "print(f\"    Clase 0: {(y_train_smote==0).sum():,}\")\n",
    "print(f\"    Clase 1: {(y_train_smote==1).sum():,}\")\n",
    "print(f\"    Ratio: 1:1.0\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 3: ENTRENAR MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[3/5] Entrenando modelos...\")\n",
    "\n",
    "# Modelo A: Baseline (sin balanceo)\n",
    "print(\"\\n  [A] Modelo baseline (sin balanceo)...\")\n",
    "\n",
    "model_baseline = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_baseline.fit(X_train, y_train)\n",
    "print(f\"      Entrenado\")\n",
    "\n",
    "# Predicciones baseline\n",
    "y_pred_base = model_baseline.predict(X_test)\n",
    "y_proba_base = model_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Modelo B: Con SMOTE\n",
    "print(\"\\n  [B] Modelo con SMOTE (balanceado)...\")\n",
    "\n",
    "model_smote = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "print(f\"      Entrenado\")\n",
    "\n",
    "# Predicciones SMOTE\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "y_proba_smote = model_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 4: EVALUACION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[4/5] Evaluando modelos...\")\n",
    "\n",
    "# Evaluación baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO BASELINE (Sin balanceo)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_base, digits=4))\n",
    "acc_base = accuracy_score(y_test, y_pred_base)\n",
    "auc_base = roc_auc_score(y_test, y_proba_base)\n",
    "print(f\"Accuracy: {acc_base:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_base:.4f}\")\n",
    "\n",
    "# Evaluación SMOTE\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO CON SMOTE (Balanceado)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_smote, digits=4))\n",
    "acc_smote = accuracy_score(y_test, y_pred_smote)\n",
    "auc_smote = roc_auc_score(y_test, y_proba_smote)\n",
    "print(f\"Accuracy: {acc_smote:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_smote:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 5: VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[5/5] Generando visualizaciones...\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# 1. Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm_base = confusion_matrix(y_test, y_pred_base)\n",
    "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Fracaso', 'Éxito'], yticklabels=['Fracaso', 'Éxito'])\n",
    "axes[0].set_title('Confusion Matrix - Baseline\\n(Sin balanceo)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicho', fontweight='bold')\n",
    "axes[0].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "sns.heatmap(cm_smote, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Fracaso', 'Éxito'], yticklabels=['Fracaso', 'Éxito'])\n",
    "axes[1].set_title('Confusion Matrix - SMOTE\\n(Con balanceo)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicho', fontweight='bold')\n",
    "axes[1].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/01_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  01_confusion_matrices.png\")\n",
    "\n",
    "# 2. ROC Curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_proba_base)\n",
    "fpr_smote, tpr_smote, _ = roc_curve(y_test, y_proba_smote)\n",
    "\n",
    "ax.plot(fpr_base, tpr_base, label=f'Baseline (AUC={auc_base:.3f})', \n",
    "        linewidth=2.5, color='#3498db')\n",
    "ax.plot(fpr_smote, tpr_smote, label=f'SMOTE (AUC={auc_smote:.3f})', \n",
    "        linewidth=2.5, color='#2ecc71')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate (FPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate (TPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Comparación de Modelos', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/02_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"02_roc_curves.png\")\n",
    "\n",
    "# 3. Feature Importance\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 12))\n",
    "\n",
    "importance = model_smote.feature_importances_\n",
    "indices = np.argsort(importance)[-20:]  # Top 20\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(indices)))\n",
    "\n",
    "ax.barh(range(len(indices)), importance[indices], color=colors)\n",
    "ax.set_yticks(range(len(indices)))\n",
    "ax.set_yticklabels([feature_names[i] for i in indices], fontsize=10)\n",
    "ax.set_xlabel('Importancia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Features Más Importantes\\n(Modelo SMOTE)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/03_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"03_feature_importance.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# GUARDAR MODELOS Y RESULTADOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GUARDANDO MODELOS Y RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Guardar modelos\n",
    "with open(f'{OUTPUT_DIR}/model_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(model_baseline, f)\n",
    "print(f\" model_baseline.pkl\")\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/model_smote.pkl', 'wb') as f:\n",
    "    pickle.dump(model_smote, f)\n",
    "print(f\"  model_smote.pkl\")\n",
    "\n",
    "# Guardar métricas\n",
    "metricas = {\n",
    "    'baseline': {\n",
    "        'accuracy': acc_base,\n",
    "        'roc_auc': auc_base,\n",
    "        'confusion_matrix': cm_base.tolist()\n",
    "    },\n",
    "    'smote': {\n",
    "        'accuracy': acc_smote,\n",
    "        'roc_auc': auc_smote,\n",
    "        'confusion_matrix': cm_smote.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/metricas.pkl', 'wb') as f:\n",
    "    pickle.dump(metricas, f)\n",
    "print(f\"  ✓ metricas.pkl\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model_smote.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_df.to_csv(f'{OUTPUT_DIR}/feature_importance.csv', index=False)\n",
    "print(f\"  ✓ feature_importance.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESUMEN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODELO BASELINE (Sin balanceo):\n",
    "  Accuracy:  {acc_base:.4f}\n",
    "  ROC-AUC:   {auc_base:.4f}\n",
    "\n",
    "MODELO SMOTE (Balanceado): ⭐\n",
    "  Accuracy:  {acc_smote:.4f}\n",
    "  ROC-AUC:   {auc_smote:.4f}\n",
    "  \n",
    "Mejora con SMOTE:\n",
    "  Accuracy:  {(acc_smote-acc_base)*100:+.2f}%\n",
    "  ROC-AUC:   {(auc_smote-auc_base)*100:+.2f}%\n",
    "\n",
    "Top 5 Features Más Importantes:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:45s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Archivos generados en '{OUTPUT_DIR}/':\n",
    "  Modelos:\n",
    "    - model_baseline.pkl\n",
    "    - model_smote.pkl ⭐ (USAR ESTE)\n",
    "  \n",
    "  Métricas:\n",
    "    - metricas.pkl\n",
    "    - feature_importance.csv\n",
    "  \n",
    "  Visualizaciones:\n",
    "    - 01_confusion_matrices.png\n",
    "    - 02_roc_curves.png\n",
    "    - 03_feature_importance.png\n",
    "\n",
    "✅ MODELADO BASELINE COMPLETADO\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SIGUIENTE PASO: Análisis SHAP (explicabilidad)\")\n",
    "print(\"  (Te lo generaré cuando estés listo)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed9a99d-9c2f-4054-89dc-b28ada8d68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/lib/python3.12/site-packages (0.12.3)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in /opt/anaconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /opt/anaconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.7.1)\n",
      "Collecting sklearn-compat<0.2,>=0.1.5 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Downloading imbalanced_learn-0.14.1-py3-none-any.whl (235 kB)\n",
      "Downloading sklearn_compat-0.1.5-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "\u001b[2K  Attempting uninstall: imbalanced-learn\n",
      "\u001b[2K    Found existing installation: imbalanced-learn 0.12.3\n",
      "\u001b[2K    Uninstalling imbalanced-learn-0.12.3:\n",
      "\u001b[2K      Successfully uninstalled imbalanced-learn-0.12.3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [imbalanced-learn][imbalanced-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imbalanced-learn-0.14.1 sklearn-compat-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25fcc922-ce13-495d-ad45-a57bef184808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO BASELINE: LightGBM + SMOTE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Cargando datos preprocesados...\n",
      "  ✓ Cargado desde pickle\n",
      "\n",
      "  Train: (54961, 192)\n",
      "  Test: (13741, 192)\n",
      "  Features: 192\n",
      "\n",
      "  Distribución Train:\n",
      "    Clase 0: 41,280 (75.1%)\n",
      "    Clase 1: 13,681 (24.9%)\n",
      "\n",
      "[2/5] Balanceo con SMOTE...\n",
      "\n",
      "  Antes de SMOTE:\n",
      "    Clase 0: 41,280\n",
      "    Clase 1: 13,681\n",
      "    Ratio: 1:3.02\n",
      "\n",
      "  ⚠️ Error con SMOTE: 'SMOTE' object has no attribute '_validate_data'\n",
      "  Usando sobremuestreo manual...\n",
      "\n",
      "  Después de sobremuestreo manual:\n",
      "    Clase 0: 41,280\n",
      "    Clase 1: 41,280\n",
      "    Ratio: 1:1.0\n",
      "  ✓ Sobremuestreo manual aplicado\n",
      "\n",
      "[3/5] Entrenando modelos...\n",
      "\n",
      "  [A] Modelo baseline (sin balanceo)...\n",
      "      ✓ Entrenado\n",
      "\n",
      "  [B] Modelo con SMOTE (balanceado)...\n",
      "      ✓ Entrenado\n",
      "\n",
      "[4/5] Evaluando modelos...+++++++++\n",
      "\n",
      "================================================================================\n",
      "MODELO BASELINE (Sin balanceo)\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000     10321\n",
      "           1     1.0000    1.0000    1.0000      3420\n",
      "\n",
      "    accuracy                         1.0000     13741\n",
      "   macro avg     1.0000    1.0000    1.0000     13741\n",
      "weighted avg     1.0000    1.0000    1.0000     13741\n",
      "\n",
      "Accuracy: 1.0000\n",
      "ROC-AUC:  1.0000\n",
      "\n",
      "================================================================================\n",
      "MODELO CON SMOTE (Balanceado)\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000     10321\n",
      "           1     1.0000    1.0000    1.0000      3420\n",
      "\n",
      "    accuracy                         1.0000     13741\n",
      "   macro avg     1.0000    1.0000    1.0000     13741\n",
      "weighted avg     1.0000    1.0000    1.0000     13741\n",
      "\n",
      "Accuracy: 1.0000\n",
      "ROC-AUC:  1.0000\n",
      "\n",
      "[5/5] Generando visualizaciones...\n",
      "  ✓ 01_confusion_matrices.png\n",
      "  ✓ 02_roc_curves.png\n",
      "  ✓ 03_feature_importance.png\n",
      "\n",
      "================================================================================\n",
      "GUARDANDO MODELOS Y RESULTADOS\n",
      "================================================================================\n",
      "  ✓ model_baseline.pkl\n",
      "  ✓ model_smote.pkl\n",
      "  ✓ metricas.pkl\n",
      "  ✓ feature_importance.csv\n",
      "\n",
      "================================================================================\n",
      "RESUMEN FINAL\n",
      "================================================================================\n",
      "\n",
      "MODELO BASELINE (Sin balanceo):\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "\n",
      "MODELO SMOTE (Balanceado): ⭐\n",
      "  Accuracy:  1.0000\n",
      "  ROC-AUC:   1.0000\n",
      "  \n",
      "Mejora con SMOTE:\n",
      "  Accuracy:  +0.00%\n",
      "  ROC-AUC:   -0.00%\n",
      "\n",
      "Top 5 Features Más Importantes:\n",
      "\n",
      "  ingresos_totales_declarados                  : 576.0000\n",
      "  costo_materia_prima                          : 525.0000\n",
      "  sector_economico                             : 494.0000\n",
      "  ventas_mes_anterior                          : 401.0000\n",
      "  num_canales_digitales                        : 387.0000\n",
      "\n",
      "Archivos generados en 'output_ml/':\n",
      "  Modelos:\n",
      "    - model_baseline.pkl\n",
      "    - model_smote.pkl ⭐ (USAR ESTE)\n",
      "  \n",
      "  Métricas:\n",
      "    - metricas.pkl\n",
      "    - feature_importance.csv\n",
      "  \n",
      "  Visualizaciones:\n",
      "    - 01_confusion_matrices.png\n",
      "    - 02_roc_curves.png\n",
      "    - 03_feature_importance.png\n",
      "\n",
      "MODELADO BASELINE COMPLETADO\n",
      "\n",
      "================================================================================\n",
      "SIGUIENTE PASO: Análisis SHAP (explicabilidad)\n",
      "  (Te lo generaré cuando estés listo)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MODELO BASELINE: LightGBM + SMOTE\n",
    "Entrenar, evaluar y guardar modelos\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, accuracy_score)\n",
    "from sklearn.utils import resample\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except:\n",
    "    SMOTE = None\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODELO BASELINE: LightGBM + SMOTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "OUTPUT_DIR = 'output_ml'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 1: CARGAR DATOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[1/5] Cargando datos preprocesados...\")\n",
    "\n",
    "# Intentar cargar desde pickle primero\n",
    "try:\n",
    "    with open(f'{OUTPUT_DIR}/train_test_split.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "    feature_names = data['feature_names']\n",
    "    \n",
    "    print(f\"  ✓ Cargado desde pickle\")\n",
    "except:\n",
    "    # Cargar desde CSV\n",
    "    print(f\"  Cargando desde CSV...\")\n",
    "    X_train = pd.read_csv(f'{OUTPUT_DIR}/X_train.csv')\n",
    "    X_test = pd.read_csv(f'{OUTPUT_DIR}/X_test.csv')\n",
    "    y_train = pd.read_csv(f'{OUTPUT_DIR}/y_train.csv').squeeze()\n",
    "    y_test = pd.read_csv(f'{OUTPUT_DIR}/y_test.csv').squeeze()\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    print(f\"  ✓ Cargado desde CSV\")\n",
    "\n",
    "print(f\"\\n  Train: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\n  Distribución Train:\")\n",
    "print(f\"    Clase 0: {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"    Clase 1: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 2: BALANCEO CON SMOTE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[2/5] Balanceo con SMOTE...\")\n",
    "\n",
    "print(f\"\\n  Antes de SMOTE:\")\n",
    "print(f\"    Clase 0: {(y_train==0).sum():,}\")\n",
    "print(f\"    Clase 1: {(y_train==1).sum():,}\")\n",
    "print(f\"    Ratio: 1:{(y_train==0).sum()/(y_train==1).sum():.2f}\")\n",
    "\n",
    "# Aplicar SMOTE con manejo de errores\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n  Después de SMOTE:\")\n",
    "    print(f\"    Clase 0: {(y_train_smote==0).sum():,}\")\n",
    "    print(f\"    Clase 1: {(y_train_smote==1).sum():,}\")\n",
    "    print(f\"    Ratio: 1:1.0\")\n",
    "    print(f\"  ✓ SMOTE aplicado correctamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n  ⚠️ Error con SMOTE: {e}\")\n",
    "    print(f\"  Usando sobremuestreo manual...\")\n",
    "    \n",
    "    # Alternativa: Sobremuestreo manual\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # Separar clases\n",
    "    X_train_0 = X_train[y_train == 0]\n",
    "    X_train_1 = X_train[y_train == 1]\n",
    "    y_train_0 = y_train[y_train == 0]\n",
    "    y_train_1 = y_train[y_train == 1]\n",
    "    \n",
    "    # Sobremuestrear clase minoritaria\n",
    "    X_train_1_upsampled = resample(X_train_1, \n",
    "                                     replace=True,\n",
    "                                     n_samples=len(X_train_0),\n",
    "                                     random_state=42)\n",
    "    y_train_1_upsampled = resample(y_train_1,\n",
    "                                     replace=True,\n",
    "                                     n_samples=len(y_train_0),\n",
    "                                     random_state=42)\n",
    "    \n",
    "    # Combinar\n",
    "    X_train_smote = pd.concat([X_train_0, X_train_1_upsampled])\n",
    "    y_train_smote = pd.concat([y_train_0, y_train_1_upsampled])\n",
    "    \n",
    "    print(f\"\\n  Después de sobremuestreo manual:\")\n",
    "    print(f\"    Clase 0: {(y_train_smote==0).sum():,}\")\n",
    "    print(f\"    Clase 1: {(y_train_smote==1).sum():,}\")\n",
    "    print(f\"    Ratio: 1:1.0\")\n",
    "    print(f\"  ✓ Sobremuestreo manual aplicado\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 3: ENTRENAR MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[3/5] Entrenando modelos...\")\n",
    "\n",
    "# Modelo A: Baseline (sin balanceo)\n",
    "print(\"\\n  [A] Modelo baseline (sin balanceo)...\")\n",
    "\n",
    "model_baseline = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_baseline.fit(X_train, y_train)\n",
    "print(f\"      ✓ Entrenado\")\n",
    "\n",
    "# Predicciones baseline\n",
    "y_pred_base = model_baseline.predict(X_test)\n",
    "y_proba_base = model_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Modelo B: Con SMOTE\n",
    "print(\"\\n  [B] Modelo con SMOTE (balanceado)...\")\n",
    "\n",
    "model_smote = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "print(f\"      ✓ Entrenado\")\n",
    "\n",
    "# Predicciones SMOTE\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "y_proba_smote = model_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 4: EVALUACION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[4/5] Evaluando modelos...+++++++++\")\n",
    "\n",
    "# Evaluación baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO BASELINE (Sin balanceo)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_base, digits=4))\n",
    "acc_base = accuracy_score(y_test, y_pred_base)\n",
    "auc_base = roc_auc_score(y_test, y_proba_base)\n",
    "print(f\"Accuracy: {acc_base:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_base:.4f}\")\n",
    "\n",
    "# Evaluación SMOTE\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO CON SMOTE (Balanceado)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_smote, digits=4))\n",
    "acc_smote = accuracy_score(y_test, y_pred_smote)\n",
    "auc_smote = roc_auc_score(y_test, y_proba_smote)\n",
    "print(f\"Accuracy: {acc_smote:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_smote:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 5: VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[5/5] Generando visualizaciones...\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# 1. Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm_base = confusion_matrix(y_test, y_pred_base)\n",
    "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Fracaso', 'Éxito'], yticklabels=['Fracaso', 'Éxito'])\n",
    "axes[0].set_title('Confusion Matrix - Baseline\\n(Sin balanceo)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicho', fontweight='bold')\n",
    "axes[0].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "sns.heatmap(cm_smote, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Fracaso', 'Éxito'], yticklabels=['Fracaso', 'Éxito'])\n",
    "axes[1].set_title('Confusion Matrix - SMOTE\\n(Con balanceo)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicho', fontweight='bold')\n",
    "axes[1].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/01_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 01_confusion_matrices.png\")\n",
    "\n",
    "# 2. ROC Curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_proba_base)\n",
    "fpr_smote, tpr_smote, _ = roc_curve(y_test, y_proba_smote)\n",
    "\n",
    "ax.plot(fpr_base, tpr_base, label=f'Baseline (AUC={auc_base:.3f})', \n",
    "        linewidth=2.5, color='#3498db')\n",
    "ax.plot(fpr_smote, tpr_smote, label=f'SMOTE (AUC={auc_smote:.3f})', \n",
    "        linewidth=2.5, color='#2ecc71')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate (FPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate (TPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Comparación de Modelos', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/02_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 02_roc_curves.png\")\n",
    "\n",
    "# 3. Feature Importance\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 12))\n",
    "\n",
    "importance = model_smote.feature_importances_\n",
    "indices = np.argsort(importance)[-20:]  # Top 20\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(indices)))\n",
    "\n",
    "ax.barh(range(len(indices)), importance[indices], color=colors)\n",
    "ax.set_yticks(range(len(indices)))\n",
    "ax.set_yticklabels([feature_names[i] for i in indices], fontsize=10)\n",
    "ax.set_xlabel('Importancia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Features Más Importantes\\n(Modelo SMOTE)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/03_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 03_feature_importance.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# GUARDAR MODELOS Y RESULTADOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GUARDANDO MODELOS Y RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Guardar modelos\n",
    "with open(f'{OUTPUT_DIR}/model_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(model_baseline, f)\n",
    "print(f\"  ✓ model_baseline.pkl\")\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/model_smote.pkl', 'wb') as f:\n",
    "    pickle.dump(model_smote, f)\n",
    "print(f\"  ✓ model_smote.pkl\")\n",
    "\n",
    "# Guardar métricas\n",
    "metricas = {\n",
    "    'baseline': {\n",
    "        'accuracy': acc_base,\n",
    "        'roc_auc': auc_base,\n",
    "        'confusion_matrix': cm_base.tolist()\n",
    "    },\n",
    "    'smote': {\n",
    "        'accuracy': acc_smote,\n",
    "        'roc_auc': auc_smote,\n",
    "        'confusion_matrix': cm_smote.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/metricas.pkl', 'wb') as f:\n",
    "    pickle.dump(metricas, f)\n",
    "print(f\"  ✓ metricas.pkl\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model_smote.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_df.to_csv(f'{OUTPUT_DIR}/feature_importance.csv', index=False)\n",
    "print(f\"  ✓ feature_importance.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESUMEN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODELO BASELINE (Sin balanceo):\n",
    "  Accuracy:  {acc_base:.4f}\n",
    "  ROC-AUC:   {auc_base:.4f}\n",
    "\n",
    "MODELO SMOTE (Balanceado): ⭐\n",
    "  Accuracy:  {acc_smote:.4f}\n",
    "  ROC-AUC:   {auc_smote:.4f}\n",
    "  \n",
    "Mejora con SMOTE:\n",
    "  Accuracy:  {(acc_smote-acc_base)*100:+.2f}%\n",
    "  ROC-AUC:   {(auc_smote-auc_base)*100:+.2f}%\n",
    "\n",
    "Top 5 Features Más Importantes:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:45s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Archivos generados en '{OUTPUT_DIR}/':\n",
    "  Modelos:\n",
    "    - model_baseline.pkl\n",
    "    - model_smote.pkl ⭐ (USAR ESTE)\n",
    "  \n",
    "  Métricas:\n",
    "    - metricas.pkl\n",
    "    - feature_importance.csv\n",
    "  \n",
    "  Visualizaciones:\n",
    "    - 01_confusion_matrices.png\n",
    "    - 02_roc_curves.png\n",
    "    - 03_feature_importance.png\n",
    "\n",
    "MODELADO BASELINE COMPLETADO\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SIGUIENTE PASO: Análisis SHAP (explicabilidad)\")\n",
    "print(\"  (Te lo generaré cuando estés listo)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194a80a6-2a36-4535-878b-585ffa9afc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETECCION DE DATA LEAKAGE\n",
      "================================================================================\n",
      "\n",
      "[1/4] Cargando dataset...\n",
      "  Dimensiones: (68702, 196)\n",
      "  Variable objetivo: exito_ingresos\n",
      "  Features: 195\n",
      "\n",
      "[2/4] Detectando correlación perfecta con target...\n",
      "\n",
      "  ⚠️ COLUMNAS CON CORRELACION PERFECTA (>95%):\n",
      "    ingresos_totales_declarados_log                   : 0.9952\n",
      "\n",
      "[3/4] Detectando columnas sospechosas de leakage...\n",
      "\n",
      "  ⚠️ COLUMNAS SOSPECHOSAS (contienen keywords):\n",
      "    - exito_compuesto\n",
      "      Valores únicos: 2\n",
      "      Distribución: {0: 51601, 1: 17101}\n",
      "    - indice_exito\n",
      "      Valores únicos: 79\n",
      "      Distribución: {45.0: 26373, 52.5: 16603, 40.0: 9577}\n",
      "\n",
      "[4/4] Eliminando columnas problemáticas...\n",
      "\n",
      "  Eliminando 12 columnas:\n",
      "    - exito_compuesto\n",
      "    - emprendedor_oportunidad\n",
      "    - formalidad_fiscal\n",
      "    - indice_competitividad\n",
      "    - id_micronegocio\n",
      "    - formalidad_laboral\n",
      "    - ingresos_subsidios\n",
      "    - ingresos_totales_declarados_log\n",
      "    - indice_exito\n",
      "    - emprendedor_necesidad\n",
      "    ... y 2 más\n",
      "\n",
      "  Features antes: 195\n",
      "  Features después: 183\n",
      "\n",
      "  ✓ Guardado: dataset_ml_sin_leakage.csv\n",
      "\n",
      "================================================================================\n",
      "DATASET LIMPIO SIN LEAKAGE\n",
      "================================================================================\n",
      "\n",
      "Columnas eliminadas: 12\n",
      "Features finales: 183\n",
      "\n",
      "SIGUIENTE PASO:\n",
      "1. Renombrar dataset:\n",
      "   mv output_fusion/dataset_ml_sin_leakage.csv output_fusion/dataset_ml_clean.csv\n",
      "\n",
      "2. Re-ejecutar preprocesamiento:\n",
      "   python 01_preprocesamiento.py\n",
      "\n",
      "3. Re-entrenar modelo:\n",
      "   python 02_modelo_baseline.py\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "DETECTAR Y CORREGIR DATA LEAKAGE\n",
    "Identificar columnas que causan overfitting\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETECCION DE DATA LEAKAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# CARGAR DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[1/4] Cargando dataset...\")\n",
    "\n",
    "df = pd.read_csv('FUSION EMICRON 2024 + GEIH 2023/dataset_ml_clean_fixed.csv')\n",
    "print(f\"  Dimensiones: {df.shape}\")\n",
    "\n",
    "# Verificar variable objetivo\n",
    "if 'exito_ingresos' in df.columns:\n",
    "    target_col = 'exito_ingresos'\n",
    "    y = df[target_col]\n",
    "    X = df.drop(target_col, axis=1)\n",
    "else:\n",
    "    print(\"  ERROR: No se encontró 'exito_ingresos'\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"  Variable objetivo: {target_col}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DETECTAR CORRELACION PERFECTA CON TARGET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[2/4] Detectando correlación perfecta con target...\")\n",
    "\n",
    "# Solo variables numéricas\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "correlaciones = []\n",
    "for col in num_cols:\n",
    "    try:\n",
    "        corr = X[col].corr(y)\n",
    "        if abs(corr) > 0.95:  # Correlación > 95%\n",
    "            correlaciones.append((col, corr))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if correlaciones:\n",
    "    print(f\"\\n  ⚠️ COLUMNAS CON CORRELACION PERFECTA (>95%):\")\n",
    "    for col, corr in sorted(correlaciones, key=lambda x: abs(x[1]), reverse=True):\n",
    "        print(f\"    {col:50s}: {corr:.4f}\")\n",
    "else:\n",
    "    print(f\"  ✓ No se encontraron correlaciones perfectas\")\n",
    "\n",
    "# =============================================================================\n",
    "# DETECTAR COLUMNAS SOSPECHOSAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[3/4] Detectando columnas sospechosas de leakage...\")\n",
    "\n",
    "# Palabras clave sospechosas\n",
    "keywords_sospechosas = ['exito', 'objetivo', 'target', 'indice_exito', \n",
    "                        'exito_compuesto', 'resultado', 'outcome']\n",
    "\n",
    "cols_sospechosas = []\n",
    "for col in X.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(kw in col_lower for kw in keywords_sospechosas):\n",
    "        cols_sospechosas.append(col)\n",
    "\n",
    "if cols_sospechosas:\n",
    "    print(f\"\\n  ⚠️ COLUMNAS SOSPECHOSAS (contienen keywords):\")\n",
    "    for col in cols_sospechosas:\n",
    "        print(f\"    - {col}\")\n",
    "        if col in X.columns:\n",
    "            # Ver distribución\n",
    "            print(f\"      Valores únicos: {X[col].nunique()}\")\n",
    "            print(f\"      Distribución: {X[col].value_counts().head(3).to_dict()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ELIMINAR COLUMNAS PROBLEMÁTICAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[4/4] Eliminando columnas problemáticas...\")\n",
    "\n",
    "cols_eliminar = []\n",
    "\n",
    "# 1. Eliminar correlaciones perfectas\n",
    "if correlaciones:\n",
    "    cols_eliminar.extend([col for col, _ in correlaciones])\n",
    "\n",
    "# 2. Eliminar columnas sospechosas\n",
    "cols_eliminar.extend(cols_sospechosas)\n",
    "\n",
    "# 3. Eliminar IDs y variables no predictivas\n",
    "cols_ids = [c for c in X.columns if any(x in c.lower() for x in ['id', 'secuencia', 'orden'])]\n",
    "cols_eliminar.extend(cols_ids)\n",
    "\n",
    "# 4. Eliminar indice_exito si existe (es derivada de target)\n",
    "if 'indice_exito' in X.columns:\n",
    "    cols_eliminar.append('indice_exito')\n",
    "\n",
    "# Eliminar duplicados\n",
    "cols_eliminar = list(set(cols_eliminar))\n",
    "\n",
    "if cols_eliminar:\n",
    "    print(f\"\\n  Eliminando {len(cols_eliminar)} columnas:\")\n",
    "    for col in cols_eliminar[:10]:\n",
    "        print(f\"    - {col}\")\n",
    "    if len(cols_eliminar) > 10:\n",
    "        print(f\"    ... y {len(cols_eliminar)-10} más\")\n",
    "    \n",
    "    # Filtrar solo las que existen\n",
    "    cols_eliminar_existentes = [c for c in cols_eliminar if c in X.columns]\n",
    "    \n",
    "    X_limpio = X.drop(columns=cols_eliminar_existentes)\n",
    "    \n",
    "    print(f\"\\n  Features antes: {X.shape[1]}\")\n",
    "    print(f\"  Features después: {X_limpio.shape[1]}\")\n",
    "    \n",
    "    # Guardar dataset limpio\n",
    "    df_limpio = X_limpio.copy()\n",
    "    df_limpio[target_col] = y\n",
    "    \n",
    "    df_limpio.to_csv('FUSION EMICRON 2024 + GEIH 2023/dataset_ml_sin_leakage.csv', index=False)\n",
    "    print(f\"\\n  ✓ Guardado: dataset_ml_sin_leakage.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET LIMPIO SIN LEAKAGE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\"\"\n",
    "Columnas eliminadas: {len(cols_eliminar_existentes)}\n",
    "Features finales: {X_limpio.shape[1]}\n",
    "\n",
    "SIGUIENTE PASO:\n",
    "1. Renombrar dataset:\n",
    "   mv output_fusion/dataset_ml_sin_leakage.csv output_fusion/dataset_ml_clean.csv\n",
    "\n",
    "2. Re-ejecutar preprocesamiento:\n",
    "   python 01_preprocesamiento.py\n",
    "\n",
    "3. Re-entrenar modelo:\n",
    "   python 02_modelo_baseline.py\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\\n   No se encontraron columnas problemáticas evidentes\")\n",
    "    print(f\"\\n  El problema puede ser más sutil. Revisa:\")\n",
    "    print(f\"    - ¿Cómo se calculó 'exito_ingresos'?\")\n",
    "    print(f\"    - ¿Hay duplicados entre train y test?\")\n",
    "    print(f\"    - ¿Las features contienen información del futuro?\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2542d9b8-ad81-40e4-806f-b5693e85f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO FINAL: PREDICCION DE FORMALIZACION\n",
      "================================================================================\n",
      "\n",
      "[1/5] Cargando datos y creando variable objetivo...\n",
      "  Dataset: (68702, 86)\n",
      "  ✓ Se eliminaron 8 variables de filtración (Data Leakage).\n",
      "  Dataset Final: (68702, 76)\n",
      "  Target: exito_formalizacion\n",
      "\n",
      "  Distribución formalidad_laboral original:\n",
      "formalidad_laboral\n",
      "0    60749\n",
      "1     5995\n",
      "2     1958\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  Nueva variable objetivo 'exito_formalizacion':\n",
      "exito_formalizacion\n",
      "0    60749\n",
      "1     7953\n",
      "Name: count, dtype: int64\n",
      "  Balance: 11.6% formal / 88.4% informal\n",
      "\n",
      "  Features: 84\n",
      "  Target: exito_formalizacion\n",
      "\n",
      "[2/5] Preprocesando datos...\n",
      "  ✓ Missing manejados\n",
      "  ✓ Categóricas codificadas: 1\n",
      "\n",
      "  Train: (54961, 84)\n",
      "  Test: (13741, 84)\n",
      "  Balance train: 11.6% / 88.4%\n",
      "\n",
      "[3/5] Balanceando clases...\n",
      "  Antes:\n",
      "    Clase 0 (informal): 48,599\n",
      "    Clase 1 (formal): 6,362\n",
      "  Después:\n",
      "    Clase 0: 48,599\n",
      "    Clase 1: 48,599\n",
      "\n",
      "[4/5] Entrenando modelos...\n",
      "\n",
      "  [A] Modelo baseline (sin balanceo)...\n",
      "      ✓ Entrenado\n",
      "\n",
      "  [B] Modelo balanceado...\n",
      "      ✓ Entrenado\n",
      "\n",
      "[5/5] Evaluando modelos...\n",
      "\n",
      "================================================================================\n",
      "MODELO BASELINE (Sin balanceo)\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Informal     0.9724    0.9898    0.9810     12150\n",
      "      Formal     0.9098    0.7857    0.8432      1591\n",
      "\n",
      "    accuracy                         0.9662     13741\n",
      "   macro avg     0.9411    0.8877    0.9121     13741\n",
      "weighted avg     0.9652    0.9662    0.9651     13741\n",
      "\n",
      "Accuracy: 0.9662\n",
      "ROC-AUC:  0.9827\n",
      "\n",
      "================================================================================\n",
      "MODELO BALANCEADO ⭐\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Informal     0.9893    0.9476    0.9680     12150\n",
      "      Formal     0.6971    0.9214    0.7937      1591\n",
      "\n",
      "    accuracy                         0.9445     13741\n",
      "   macro avg     0.8432    0.9345    0.8808     13741\n",
      "weighted avg     0.9554    0.9445    0.9478     13741\n",
      "\n",
      "Accuracy: 0.9445\n",
      "ROC-AUC:  0.9845\n",
      "\n",
      "================================================================================\n",
      "VALIDACION CRUZADA (5-fold)\n",
      "================================================================================\n",
      "  ROC-AUC por fold: [0.98291058 0.98622608 0.98412928 0.98341891 0.98385763]\n",
      "  Media: 0.9841 (+/- 0.0011)\n",
      "\n",
      "================================================================================\n",
      "GENERANDO VISUALIZACIONES\n",
      "================================================================================\n",
      "  ✓ 01_confusion_matrices.png\n",
      "  ✓ 02_roc_curves.png\n",
      "  ✓ 03_feature_importance.png\n",
      "\n",
      "================================================================================\n",
      "GUARDANDO RESULTADOS\n",
      "================================================================================\n",
      "  ✓ Modelos guardados\n",
      "  ✓ Métricas guardadas\n",
      "   feature_importance.csv\n",
      "\n",
      "================================================================================\n",
      "RESUMEN FINAL\n",
      "================================================================================\n",
      "\n",
      "PREGUNTA DE INVESTIGACION:\n",
      "  ¿Qué factores predicen la formalización de micronegocios?\n",
      "\n",
      "VARIABLE OBJETIVO:\n",
      "  exito_formalizacion (0=Informal, 1=Formal)\n",
      "  Balance: 11.6% formal / 88.4% informal\n",
      "\n",
      "MODELO BASELINE (Sin balanceo):\n",
      "  Accuracy:  0.9662\n",
      "  ROC-AUC:   0.9827\n",
      "  \n",
      "MODELO BALANCEADO :\n",
      "  Accuracy:  0.9445\n",
      "  ROC-AUC:   0.9845\n",
      "  CV ROC-AUC: 0.9841 (+/- 0.0011)\n",
      "\n",
      "Top 5 Predictores de Formalización:\n",
      "\n",
      "  antiguedad_negocio                           : 528.0000\n",
      "  codigo_departamento                          : 413.0000\n",
      "  activo_intangibles                           : 411.0000\n",
      "  activo_herramientas                          : 409.0000\n",
      "  activos_calc                                 : 400.0000\n",
      "\n",
      "INTERPRETACION:\n",
      "  - Métricas realistas (no 100%)\n",
      "  - Modelo aprende patrones reales\n",
      "  - Sin data leakage\n",
      "  - Resultados aplicables a política pública\n",
      "\n",
      "ARCHIVOS EN 'output_ml_final/':\n",
      "  - model_balanceado.pkl \n",
      "  - metricas.pkl\n",
      "  - feature_importance.csv\n",
      "  - 01_confusion_matrices.png\n",
      "  - 02_roc_curves.png\n",
      "  - 03_feature_importance.png\n",
      "MODELO FINAL COMPLETADO\n",
      "\n",
      "================================================================================\n",
      "SIGUIENTE PASO: Análisis SHAP (explicabilidad)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MODELO FINAL: PREDICCION DE FORMALIZACION\n",
    "Variable objetivo: Formalidad laboral (informal vs formal)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, accuracy_score)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import resample\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODELO FINAL: PREDICCION DE FORMALIZACION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "OUTPUT_DIR = 'output_ml_final'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 1: CARGAR Y CREAR VARIABLE OBJETIVO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[1/5] Cargando datos y creando variable objetivo...\")\n",
    "\n",
    "df = pd.read_csv('FUSION EMICRON 2024 + GEIH 2023/dataset_ml_sin_indices.csv')\n",
    "print(f\"  Dataset: {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#======================================\n",
    "\n",
    "\n",
    "\n",
    "# --- BLOQUE DE CORRECCIÓN: ELIMINACIÓN DE DATA LEAKAGE ---\n",
    "# Eliminamos variables que son consecuencia de ser formal o que contienen la respuesta\n",
    "# Esto evita el 1.00 de accuracy falso.\n",
    "columnas_leakage = [\n",
    "    # Registros formales (Si tiene RUT o Registro Mercantil, ya es formal por definición)\n",
    "    'registro_mercantil', 'registro_rut', 'otro_registro', 'tiene_registro_formal',\n",
    "    'formalidad_fiscal', 'formalidad_juridica',\n",
    "    # Variables de crédito que delatan formalidad\n",
    "    'num_creditos_formales', 'tiene_credito_formal', 'credito_bancario', \n",
    "    'credito_microfinanzas', 'credito_cooperativa', 'acceso_credito',\n",
    "    # Otras variables endógenas\n",
    "    'exito_ingresos' \n",
    "]\n",
    "\n",
    "# Filtrar columnas existentes para evitar errores\n",
    "leakage_to_drop = [c for c in columnas_leakage if c in df.columns]\n",
    "df_limpio = df.drop(columns=leakage_to_drop)\n",
    "\n",
    "print(f\"  ✓ Se eliminaron {len(leakage_to_drop)} variables de filtración (Data Leakage).\")\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Verificamos formalidad_laboral (que es nuestra etiqueta real)\n",
    "if 'formalidad_laboral' not in df.columns:\n",
    "    print(\"\\n  ✗ ERROR: No se encontró 'formalidad_laboral'\")\n",
    "    exit(1)\n",
    "\n",
    "# CREAR VARIABLE OBJETIVO BINARIA\n",
    "y = (df['formalidad_laboral'] >= 1).astype(int)\n",
    "\n",
    "# Definimos X usando el dataframe limpio\n",
    "X = df_limpio.drop(['formalidad_laboral'], axis=1)\n",
    "\n",
    "# Eliminamos también identificadores si existen\n",
    "cols_id = ['id_micronegocio', 'id_persona', 'id_encuesta', 'nombre_departamento', 'DIRECTORIO']\n",
    "X = X.drop(columns=[c for c in cols_id if c in X.columns], errors='ignore')\n",
    "\n",
    "print(f\"  Dataset Final: {X.shape}\")\n",
    "print(f\"  Target: exito_formalizacion\")\n",
    "\n",
    "\n",
    "\n",
    "#==========================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Verificar que existe formalidad_laboral\n",
    "if 'formalidad_laboral' not in df.columns:\n",
    "    print(\"\\n  ✗ ERROR: No se encontró 'formalidad_laboral'\")\n",
    "    exit(1)\n",
    "\n",
    "# Ver distribución original\n",
    "print(f\"\\n  Distribución formalidad_laboral original:\")\n",
    "print(df['formalidad_laboral'].value_counts().sort_index())\n",
    "\n",
    "# CREAR VARIABLE OBJETIVO BINARIA\n",
    "# 0 = Informal, 1+ = Formal (parcial o completo)\n",
    "df['exito_formalizacion'] = (df['formalidad_laboral'] >= 1).astype(int)\n",
    "\n",
    "print(f\"\\n  Nueva variable objetivo 'exito_formalizacion':\")\n",
    "print(df['exito_formalizacion'].value_counts())\n",
    "balance = df['exito_formalizacion'].mean() * 100\n",
    "print(f\"  Balance: {balance:.1f}% formal / {100-balance:.1f}% informal\")\n",
    "\n",
    "# Eliminar variables objetivo anteriores y la original\n",
    "cols_drop = ['exito_ingresos', 'formalidad_laboral']\n",
    "cols_drop = [c for c in cols_drop if c in df.columns]\n",
    "\n",
    "y = df['exito_formalizacion']\n",
    "X = df.drop(['exito_formalizacion'] + cols_drop, axis=1)\n",
    "\n",
    "print(f\"\\n  Features: {X.shape[1]}\")\n",
    "print(f\"  Target: exito_formalizacion\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 2: PREPROCESAMIENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[2/5] Preprocesando datos...\")\n",
    "\n",
    "# Manejar missing\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "for col in num_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        X[col] = X[col].fillna('Desconocido')\n",
    "    X[col] = X[col].astype('category').cat.codes\n",
    "\n",
    "print(f\"  ✓ Missing manejados\")\n",
    "print(f\"  ✓ Categóricas codificadas: {len(cat_cols)}\")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n  Train: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "print(f\"  Balance train: {y_train.mean()*100:.1f}% / {(1-y_train.mean())*100:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 3: BALANCEO CON SMOTE MANUAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[3/5] Balanceando clases...\")\n",
    "\n",
    "print(f\"  Antes:\")\n",
    "print(f\"    Clase 0 (informal): {(y_train==0).sum():,}\")\n",
    "print(f\"    Clase 1 (formal): {(y_train==1).sum():,}\")\n",
    "\n",
    "# Sobremuestreo de clase minoritaria\n",
    "X_train_0 = X_train[y_train == 0]\n",
    "X_train_1 = X_train[y_train == 1]\n",
    "y_train_0 = y_train[y_train == 0]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "X_train_1_up = resample(X_train_1, replace=True, n_samples=len(X_train_0), random_state=42)\n",
    "y_train_1_up = resample(y_train_1, replace=True, n_samples=len(y_train_0), random_state=42)\n",
    "\n",
    "X_train_bal = pd.concat([X_train_0, X_train_1_up])\n",
    "y_train_bal = pd.concat([y_train_0, y_train_1_up])\n",
    "\n",
    "print(f\"  Después:\")\n",
    "print(f\"    Clase 0: {(y_train_bal==0).sum():,}\")\n",
    "print(f\"    Clase 1: {(y_train_bal==1).sum():,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 4: ENTRENAR MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[4/5] Entrenando modelos...\")\n",
    "\n",
    "# Modelo A: Sin balanceo\n",
    "print(\"\\n  [A] Modelo baseline (sin balanceo)...\")\n",
    "\n",
    "model_baseline = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_baseline.fit(X_train, y_train)\n",
    "y_pred_base = model_baseline.predict(X_test)\n",
    "y_proba_base = model_baseline.predict_proba(X_test)[:, 1]\n",
    "print(f\"      ✓ Entrenado\")\n",
    "\n",
    "# Modelo B: Con balanceo\n",
    "print(\"\\n  [B] Modelo balanceado...\")\n",
    "\n",
    "model_balanced = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=50,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_balanced.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = model_balanced.predict(X_test)\n",
    "y_proba_bal = model_balanced.predict_proba(X_test)[:, 1]\n",
    "print(f\"      ✓ Entrenado\")\n",
    "\n",
    "# =============================================================================\n",
    "# PASO 5: EVALUACION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n[5/5] Evaluando modelos...\")\n",
    "\n",
    "# Baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO BASELINE (Sin balanceo)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_base, \n",
    "                           target_names=['Informal', 'Formal'], digits=4))\n",
    "acc_base = accuracy_score(y_test, y_pred_base)\n",
    "auc_base = roc_auc_score(y_test, y_proba_base)\n",
    "print(f\"Accuracy: {acc_base:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_base:.4f}\")\n",
    "\n",
    "# Balanceado\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO BALANCEADO ⭐\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_bal,\n",
    "                           target_names=['Informal', 'Formal'], digits=4))\n",
    "acc_bal = accuracy_score(y_test, y_pred_bal)\n",
    "auc_bal = roc_auc_score(y_test, y_proba_bal)\n",
    "print(f\"Accuracy: {acc_bal:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_bal:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDACION CRUZADA (5-fold)\")\n",
    "print(\"=\"*80)\n",
    "cv_scores = cross_val_score(model_balanced, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"  ROC-AUC por fold: {cv_scores}\")\n",
    "print(f\"  Media: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERANDO VISUALIZACIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# 1. Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm_base = confusion_matrix(y_test, y_pred_base)\n",
    "cm_bal = confusion_matrix(y_test, y_pred_bal)\n",
    "\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Informal', 'Formal'], yticklabels=['Informal', 'Formal'])\n",
    "axes[0].set_title(f'Baseline\\nAccuracy: {acc_base:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicho', fontweight='bold')\n",
    "axes[0].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "sns.heatmap(cm_bal, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Informal', 'Formal'], yticklabels=['Informal', 'Formal'])\n",
    "axes[1].set_title(f'Balanceado ⭐\\nAccuracy: {acc_bal:.3f}', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicho', fontweight='bold')\n",
    "axes[1].set_ylabel('Real', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/01_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 01_confusion_matrices.png\")\n",
    "\n",
    "# 2. ROC Curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_proba_base)\n",
    "fpr_bal, tpr_bal, _ = roc_curve(y_test, y_proba_bal)\n",
    "\n",
    "ax.plot(fpr_base, tpr_base, label=f'Baseline (AUC={auc_base:.3f})', linewidth=2.5, color='#3498db')\n",
    "ax.plot(fpr_bal, tpr_bal, label=f'Balanceado (AUC={auc_bal:.3f})', linewidth=2.5, color='#2ecc71')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Predicción de Formalización', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/02_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 02_roc_curves.png\")\n",
    "\n",
    "# 3. Feature Importance\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 12))\n",
    "\n",
    "importance = model_balanced.feature_importances_\n",
    "indices = np.argsort(importance)[-20:]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(indices)))\n",
    "\n",
    "ax.barh(range(len(indices)), importance[indices], color=colors)\n",
    "ax.set_yticks(range(len(indices)))\n",
    "ax.set_yticklabels([X.columns[i] for i in indices], fontsize=10)\n",
    "ax.set_xlabel('Importancia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Features - Predicción de Formalización', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/03_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 03_feature_importance.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# GUARDAR RESULTADOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GUARDANDO RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Modelos\n",
    "with open(f'{OUTPUT_DIR}/model_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(model_baseline, f)\n",
    "with open(f'{OUTPUT_DIR}/model_balanceado.pkl', 'wb') as f:\n",
    "    pickle.dump(model_balanced, f)\n",
    "print(f\"  ✓ Modelos guardados\")\n",
    "\n",
    "# Métricas\n",
    "metricas = {\n",
    "    'baseline': {'accuracy': acc_base, 'roc_auc': auc_base, 'cm': cm_base.tolist()},\n",
    "    'balanceado': {'accuracy': acc_bal, 'roc_auc': auc_bal, 'cm': cm_bal.tolist()},\n",
    "    'cv_scores': cv_scores.tolist()\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/metricas.pkl', 'wb') as f:\n",
    "    pickle.dump(metricas, f)\n",
    "print(f\"  ✓ Métricas guardadas\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_balanced.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_df.to_csv(f'{OUTPUT_DIR}/feature_importance.csv', index=False)\n",
    "print(f\"   feature_importance.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESUMEN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "PREGUNTA DE INVESTIGACION:\n",
    "  ¿Qué factores predicen la formalización de micronegocios?\n",
    "\n",
    "VARIABLE OBJETIVO:\n",
    "  exito_formalizacion (0=Informal, 1=Formal)\n",
    "  Balance: {balance:.1f}% formal / {100-balance:.1f}% informal\n",
    "\n",
    "MODELO BASELINE (Sin balanceo):\n",
    "  Accuracy:  {acc_base:.4f}\n",
    "  ROC-AUC:   {auc_base:.4f}\n",
    "  \n",
    "MODELO BALANCEADO :\n",
    "  Accuracy:  {acc_bal:.4f}\n",
    "  ROC-AUC:   {auc_bal:.4f}\n",
    "  CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\n",
    "\n",
    "Top 5 Predictores de Formalización:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:45s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "INTERPRETACION:\n",
    "  - Métricas realistas (no 100%)\n",
    "  - Modelo aprende patrones reales\n",
    "  - Sin data leakage\n",
    "  - Resultados aplicables a política pública\n",
    "\n",
    "ARCHIVOS EN '{OUTPUT_DIR}/':\n",
    "  - model_balanceado.pkl \n",
    "  - metricas.pkl\n",
    "  - feature_importance.csv\n",
    "  - 01_confusion_matrices.png\n",
    "  - 02_roc_curves.png\n",
    "  - 03_feature_importance.png\n",
    "MODELO FINAL COMPLETADO\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SIGUIENTE PASO: Análisis SHAP (explicabilidad)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8171aaf8-e471-42ae-a95f-1f7974aaae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERANDO GRAFICAS DE ENTRENAMIENTO Y EVALUACION\n",
      "================================================================================\n",
      " 04_entrenamiento_evaluacion.png\n",
      "  ✓ 05_analisis_umbral.png\n",
      "  06_distribucion_probabilidades.png\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACIONES ADICIONALES: ENTRENAMIENTO Y EVALUACION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERANDO GRAFICAS DE ENTRENAMIENTO Y EVALUACION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4. Curvas de Aprendizaje (Learning Curves)\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Calcular curvas de aprendizaje\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    model_balanced, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Calcular media y desviación estándar\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Gráfica izquierda: Curvas de aprendizaje\n",
    "axes[0].plot(train_sizes, train_mean, 'o-', color='#3498db', linewidth=2.5, label='Train')\n",
    "axes[0].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                      alpha=0.2, color='#3498db')\n",
    "axes[0].plot(train_sizes, val_mean, 'o-', color='#e74c3c', linewidth=2.5, label='Validation')\n",
    "axes[0].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                      alpha=0.2, color='#e74c3c')\n",
    "\n",
    "axes[0].set_xlabel('Tamaño del conjunto de entrenamiento', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('ROC-AUC Score', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Curvas de Aprendizaje\\n(Modelo Balanceado)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11, loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# Gráfica derecha: Comparación de métricas\n",
    "metrics_names = ['Accuracy', 'ROC-AUC', 'Precision\\n(Formal)', 'Recall\\n(Formal)']\n",
    "\n",
    "# Calcular precision y recall para clase formal (1)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_base = precision_score(y_test, y_pred_base, pos_label=1)\n",
    "recall_base = recall_score(y_test, y_pred_base, pos_label=1)\n",
    "precision_bal = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "baseline_scores = [acc_base, auc_base, precision_base, recall_base]\n",
    "balanced_scores = [acc_bal, auc_bal, precision_bal, recall_bal]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, baseline_scores, width, label='Baseline', \n",
    "                    color='#3498db', alpha=0.8)\n",
    "bars2 = axes[1].bar(x + width/2, balanced_scores, width, label='Balanceado', \n",
    "                    color='#2ecc71', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Métrica', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Comparación de Métricas\\n(Test Set)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics_names, fontsize=10)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/04_entrenamiento_evaluacion.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\" 04_entrenamiento_evaluacion.png\")\n",
    "\n",
    "# 5. Matriz de Métricas por Umbral (Threshold Analysis)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Calcular métricas para diferentes umbrales\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_proba_bal >= threshold).astype(int)\n",
    "    \n",
    "    if len(np.unique(y_pred_threshold)) > 1:  # Verificar que hay ambas clases\n",
    "        precision = precision_score(y_test, y_pred_threshold, pos_label=1, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred_threshold, pos_label=1, zero_division=0)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Gráfica izquierda: Precision-Recall vs Threshold\n",
    "axes[0].plot(thresholds, precisions, label='Precision', linewidth=2.5, color='#e74c3c')\n",
    "axes[0].plot(thresholds, recalls, label='Recall', linewidth=2.5, color='#3498db')\n",
    "axes[0].plot(thresholds, f1_scores, label='F1-Score', linewidth=2.5, color='#2ecc71', linestyle='--')\n",
    "\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Umbral 0.5')\n",
    "axes[0].set_xlabel('Umbral de Clasificación', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Métricas vs Umbral de Decisión', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1.0])\n",
    "\n",
    "# Gráfica derecha: Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba_bal)\n",
    "\n",
    "axes[1].plot(recall_curve, precision_curve, linewidth=2.5, color='#9b59b6')\n",
    "axes[1].fill_between(recall_curve, precision_curve, alpha=0.2, color='#9b59b6')\n",
    "\n",
    "axes[1].set_xlabel('Recall (Sensibilidad)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Curva Precision-Recall\\n(Modelo Balanceado)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 1.0])\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "\n",
    "# Agregar línea de referencia (no-skill)\n",
    "no_skill = (y_test == 1).sum() / len(y_test)\n",
    "axes[1].axhline(y=no_skill, color='gray', linestyle='--', linewidth=1.5, \n",
    "                label=f'No-skill ({no_skill:.3f})')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/05_analisis_umbral.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ✓ 05_analisis_umbral.png\")\n",
    "\n",
    "# 6. Distribución de Probabilidades Predichas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gráfica izquierda: Histograma de probabilidades\n",
    "axes[0].hist(y_proba_bal[y_test == 0], bins=50, alpha=0.6, label='Informal (Real)', \n",
    "             color='#e74c3c', density=True)\n",
    "axes[0].hist(y_proba_bal[y_test == 1], bins=50, alpha=0.6, label='Formal (Real)', \n",
    "             color='#2ecc71', density=True)\n",
    "\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle='--', linewidth=2, label='Umbral 0.5')\n",
    "axes[0].set_xlabel('Probabilidad Predicha (Formal)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Densidad', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Distribución de Probabilidades Predichas\\npor Clase Real', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfica derecha: Boxplot de probabilidades\n",
    "data_to_plot = [y_proba_bal[y_test == 0], y_proba_bal[y_test == 1]]\n",
    "bp = axes[1].boxplot(data_to_plot, labels=['Informal\\n(Real)', 'Formal\\n(Real)'],\n",
    "                     patch_artist=True, widths=0.6)\n",
    "\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', linewidth=1.5, label='Umbral 0.5')\n",
    "axes[1].set_ylabel('Probabilidad Predicha (Formal)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Distribución de Scores por Clase Real', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/06_distribucion_probabilidades.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  06_distribucion_probabilidades.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ae898-8642-4f8c-a780-23558eee59d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
